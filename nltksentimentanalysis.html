<!DOCTYPE html>
<html>
    <head>
        <title>NLTK Sentiment Analysis Tutorial</title>
    </head>
    
    <body>
        <header>
            <h1>Introduction</h1>
            <p> Natural Language Toolkit, or NLTK, is a Python library configured to analyze natural speech data. NLTK offers a wide range of features that can easily process any body of text, such as part-of-speech tagging, tokenization, and semantic reasoning. 
                One of NLTK's most popular features is its sentiment analysis tool, VADER. VADER, also known as <b>Valence Aware Dictionary for sEntiment Reasoning</b>, uses its sentiment lexicon and a blend of grammatical rules and syntactical conventions.</p>

            <h2>How does VADER work?</h2>
            <h3>Sentiment Lexicon</h3>
                <p>A sentiment lexicon is a dictionary that maps words, phrases, and symbols and matches those to a specific emotional valence. VADER's literature further explains this process:</p>
                <blockquote> "We  begin  by  constructing  a  list  inspired  by  examiningexisting well-established  sentiment  word-banks  (LIWC, ANEW, and  GI).  To  this,  we  next  incorporate  numerous lexical  features  common  to  sentiment  expression  
                    in  microblogs, including a full list of Western-style  emoticons (for example, “:-)” denotes a “smiley face” and generally indicates  positive  sentiment),  sentiment-related  acronyms and  initialisms (e.g.,  LOL  and  WTF  are  both  
                    sentiment-laden  initialisms),  and  commonly  used  slang  with  senti-ment value (e.g., “nah”, “meh   ”and “giggly”). This processprovided us with over 9,000 lexical feature candidates. Next,  we  assessed  the  general  applicability  
                    of  each  feature  candidate  to  sentiment  expressions.  We  used  a  wisdom-of-the-crowd  (WotC)  approach  (Surowiecki,  2004) to  acquire  a  valid  point  estimate  for  the  sentiment  valence (intensity)  of  each  context-free  
                    candidate  feature.  We  collected  intensity  ratings  on  each  of  our  candidate  lexical features  from  ten  independent  human  raters  (for  a  total  of 90,000+ ratings). Features were rated on a scale from '[–4] Extremely Negative' 
                    to '[4] Extremely Positive', with allowance for '[0] Neutral (or Neither, N/A)'" (Hutto & Gilbert, 2014). </blockquote>
                <p></p>
        <nav>
            <ul>
                <h2>Sections</h2>
                <li><a href="#step0">Step 0: Installing and Importing Modules</a></li>
                <li><a href="#step1">Step 1: Loading in and Cleaning Up Your CSV</a></li>
                <li><a href="#step2">Step 2: Create a Speaker Dictionary</a></li>
            </ul>
        </nav>

        </header>

        
        <main>
            <section id="step0">
                <h2>Step 0: Installing and Importing Modules</h2>
                <p>For step zero, you will need to start by installing and importing the necessary modules. For this particular analysis, you will need to download the following modules onto your terminal.</p>
                <pre><code>
                    pip install pandas
                    pip install nltk
                    # this code will download all of the necessary 
                    python3 -c "import nltk; [nltk.download(x) for x in ['vader_lexicon', 'stopwords', 'punkt', 'wordnet', 'omw-1.4']]"
                </pre></code>
                <p> Once you download those, you will need to also download our dataset from GitHub. Make sure you are downloading the <b>raw</b> dataset. You can do this directly in your terminal like this:</p>
                <pre><code> 
                wget https://raw.githubusercontent.com/favstats/demdebates2020/refs/heads/master/data/debates.csv
                </code></pre>
                <p> Use <code>python3</code> to import your modules like so:<p>
                    <pre><code>
                        import nltk
                        from nltk.sentiment.vader import SentimentIntensityAnalyzer
                        from nltk.corpus import stopwords
                        from nltk.tokenize import word_tokenize
                        from nltk.stem import WordNetLemmatizer
                        import re
                        import pandas as pd
                        import string
                    </code></pre>
                <p> Once everything is downloaded correctly, you are ready to head to step 1.</p>
                
            </section>

            <section id="step1">
                <h2>Step 1: Loading in and Cleaning Up Your CSV</h2>
                <p>Once you have imported all your modules and downloaded your data, it's time to load in the file.</p>
                <pre><code>
                 df = pd.read_csv('debates.csv')
                 csvcleaned = df[
                 (df['speech'] != 'NA') &
                 (df['type'] != 'Moderator') &
                 (~df['speaker'].isin(['Speaker 1:', 'Speaker 2:', 'speaker 6:', 'speaker 7:', 'speaker 8:', 'Unknown',
                                                                                                'Protester']))] \
                 .drop(columns=['background', 'gender', 'debate', 'day', 'type', 'order']
                 )
                 csvcleaned.to_csv('debate.csv', index=False)   
                </code></pre>

                <p>Let's break down each piece of the code:</p>
                <pre><code>df = pd.read_csv('debates.csv')</code></pre>
                <p>The variable "df" here stands for DataFrame, a type of document structure. These are commonly used with datasets that have rows and columns (e.g. spreadsheet, CSV, SQL table). We are essentially loading debates.csv in to the DataFrame.

                <pre><code>csvcleaned = df[
                (df['speech'] != 'NA') &
                (df['type'] != 'Moderator') &
                (~df['speaker'].isin(['Speaker 1:', 'Speaker 2:', 'speaker 6:', 'speaker 7:', 'speaker 8:', 'Unknown',
                'Protester']))] \
                .drop(columns=['background', 'gender', 'debate', 'day', 'type', 'order']
                )
                csvcleaned.to_csv('debate.csv', index=False)
                </pre></code>
                <p>This serves to tell the program to remove columns we don't want ('background', 'gender', 'debate', 'day', 'type', 'order'), as well as not adding any data where the row has "NA" in the speech column or 'Speaker 1:', 'Speaker 2:', 'speaker 6:', 'speaker 7:', 'speaker 8:', 'Unknown',
                'Protester.' Once that is all compiled, everything in the variable 'csvcleaned' will be added in a new CSV file called 'debate.csv'.</p>
                
            </section>

            <section id="step2">
                <h2>Third Section</h2>
                <p>paragraph on Third topic</p>
            </section>
        </main>
        <p>Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.</p>
        <footer>
            <h1>Conclusive Section With Contact Information</h1>
            <p>Last Notes Here:conclusion</p>

            <address>
                Written by<a href="mailto:youremail@example.com"> Kathleen Costa</a>
                
            </address>
        </footer>

    </body>
</html>
