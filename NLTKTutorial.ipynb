{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90d5f58-684a-406f-be60-863cceafabdd",
   "metadata": {},
   "source": [
    "# Optimized Jupyter Notebook for NLTK VADER Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c82b39-7da0-4819-95bc-2e296b65826b",
   "metadata": {},
   "source": [
    "This notebook has been optimized specifically so that you can run the entire tutorial from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f651a3c-14b5-45cd-aa04-5f04a8badc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!wget https://raw.githubusercontent.com/favstats/demdebates2020/refs/heads/master/data/debates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4f862-8583-4f11-bb35-e7531e7952fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a75500-0eb4-4f77-bfdb-7b468543689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('debates.csv')\n",
    "csvcleaned = df[\n",
    "(df['speech'] != 'NA') &\n",
    "(df['type'] != 'Moderator') &\n",
    "(~df['speaker'].isin(['Speaker 1:', 'Speaker 2:', 'speaker 6:', 'speaker 7:', 'speaker 8:', 'Unknown',                                                                                                'Protester']))] \\\n",
    ".drop(columns=['background', 'gender', 'debate', 'day', 'type', 'order']\n",
    "                 )\n",
    "csvcleaned.to_csv('debate.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2d170-75a6-48f1-8858-7f16c5a58c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerdictionary = {}\n",
    "for _, row in csvcleaned.iterrows():\n",
    "    speaker = row['speaker']\n",
    "    speech = row['speech']\n",
    "\n",
    "    if speaker not in speakerdictionary:\n",
    "        speakerdictionary[speaker] = []\n",
    "    speakerdictionary[speaker].append(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab05fc8-266f-40f2-b934-f3d23cefbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizeddictionary = {}\n",
    "    punctuation = ['...', '--', '``', \"''\"]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for speaker, speeches in speakerdictionary.items():\n",
    "        lemmatizeddictionary[speaker] = []\n",
    "        for sentence in speeches:\n",
    "            tokens = word_tokenize(str(sentence).lower())\n",
    "            tokens = [word for word in tokens if\n",
    "                      word not in string.punctuation and word not in punctuation and word != 'nan']\n",
    "            tokens = [word for word in tokens if not re.fullmatch(r\"\\s*\", word)]\n",
    "            filteredtokens = [token for token in tokens if token not in stop_words]\n",
    "            \n",
    "            lemmatized = [lemmatizer.lemmatize(word) for word in filteredtokens if word]\n",
    "            if lemmatized:\n",
    "                lemmatizeddictionary[speaker].append(lemmatized)\n",
    "    return lemmatizeddictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71b3aa-f84d-4bf1-9646-fa0b655dfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(speakerdictionary):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for speaker, speeches in speakerdictionary.items():\n",
    "        text = \" \".join([\" \".join(sentence) for sentence in speeches])\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        print(f\"{speaker}: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc8a17-2817-46c9-9c36-af74b4d96ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing()\n",
    "sentiment(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2503f-df8d-48dc-86e0-5af47f0e5d9f",
   "metadata": {},
   "source": [
    "Created by [Kathleen Costa](https://github.com/kathleenalvescosta) | Spring 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
