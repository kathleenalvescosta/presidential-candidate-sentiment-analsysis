{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90d5f58-684a-406f-be60-863cceafabdd",
   "metadata": {},
   "source": [
    "# Optimized Jupyter Notebook for NLTK VADER Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce7ba9-c58c-4b17-987c-de9e13e338f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "This notebook has been optimized specifically so that you can run the entire tutorial from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f651a3c-14b5-45cd-aa04-5f04a8badc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./miniconda3/lib/python3.12/site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "--2025-05-07 01:45:43--  https://raw.githubusercontent.com/favstats/demdebates2020/refs/heads/master/data/debates.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2081989 (2.0M) [text/plain]\n",
      "Saving to: ‘debates.csv’\n",
      "\n",
      "debates.csv         100%[===================>]   1.99M  5.61MB/s    in 0.4s    \n",
      "\n",
      "2025-05-07 01:45:44 (5.61 MB/s) - ‘debates.csv’ saved [2081989/2081989]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!wget https://raw.githubusercontent.com/favstats/demdebates2020/refs/heads/master/data/debates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc4f862-8583-4f11-bb35-e7531e7952fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/kathleenalvescosta/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/kathleenalvescosta/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kathleenalvescosta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/kathleenalvescosta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a75500-0eb4-4f77-bfdb-7b468543689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('debates.csv')\n",
    "csvcleaned = df[\n",
    "(df['speech'] != 'NA') &\n",
    "(df['type'] != 'Moderator') &\n",
    "(~df['speaker'].isin(['Speaker 1:', 'Speaker 2:', 'speaker 6:', 'speaker 7:', 'speaker 8:', 'Unknown',                                                                                                'Protester']))] \\\n",
    ".drop(columns=['background', 'gender', 'debate', 'day', 'type', 'order']\n",
    "                 )\n",
    "csvcleaned.to_csv('debate.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea2d170-75a6-48f1-8858-7f16c5a58c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerdictionary = {}\n",
    "for _, row in csvcleaned.iterrows():\n",
    "    speaker = row['speaker']\n",
    "    speech = row['speech']\n",
    "\n",
    "    if speaker not in speakerdictionary:\n",
    "        speakerdictionary[speaker] = []\n",
    "    speakerdictionary[speaker].append(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ab05fc8-266f-40f2-b934-f3d23cefbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizeddictionary = {}\n",
    "    punctuation = ['...', '--', '``', \"''\"]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for speaker, speeches in speakerdictionary.items():\n",
    "        lemmatizeddictionary[speaker] = []\n",
    "        for sentence in speeches:\n",
    "            tokens = word_tokenize(str(sentence).lower())\n",
    "            tokens = [word for word in tokens if\n",
    "                      word not in string.punctuation and word not in punctuation and word != 'nan']\n",
    "            tokens = [word for word in tokens if not re.fullmatch(r\"\\s*\", word)]\n",
    "            filteredtokens = [token for token in tokens if token not in stop_words]\n",
    "            \n",
    "            lemmatized = [lemmatizer.lemmatize(word) for word in filteredtokens if word]\n",
    "            if lemmatized:\n",
    "                lemmatizeddictionary[speaker].append(lemmatized)\n",
    "    return lemmatizeddictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f71b3aa-f84d-4bf1-9646-fa0b655dfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(speakerdictionary):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    for speaker, speeches in speakerdictionary.items():\n",
    "        text = \" \".join([\" \".join(sentence) for sentence in speeches])\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        scores = analyzer.polarity_scores(text)\n",
    "        print(f\"{speaker}: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ffc8a17-2817-46c9-9c36-af74b4d96ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elizabeth Warren: {'neg': 0.121, 'neu': 0.687, 'pos': 0.192, 'compound': 1.0}\n",
      "Amy Klobuchar: {'neg': 0.087, 'neu': 0.717, 'pos': 0.196, 'compound': 1.0}\n",
      "Beto O'Rourke: {'neg': 0.126, 'neu': 0.663, 'pos': 0.211, 'compound': 0.9998}\n",
      "Cory Booker: {'neg': 0.153, 'neu': 0.631, 'pos': 0.216, 'compound': 0.9999}\n",
      "Julian Castro: {'neg': 0.083, 'neu': 0.709, 'pos': 0.207, 'compound': 0.9999}\n",
      "Tulsi Gabbard: {'neg': 0.17, 'neu': 0.621, 'pos': 0.209, 'compound': 0.9974}\n",
      "Bill de Blasio: {'neg': 0.123, 'neu': 0.672, 'pos': 0.205, 'compound': 0.999}\n",
      "John Delaney: {'neg': 0.064, 'neu': 0.7, 'pos': 0.236, 'compound': 0.9998}\n",
      "Jay Inslee: {'neg': 0.098, 'neu': 0.639, 'pos': 0.263, 'compound': 0.9997}\n",
      "Tim Ryan: {'neg': 0.091, 'neu': 0.711, 'pos': 0.198, 'compound': 0.9995}\n",
      "Bernie Sanders: {'neg': 0.131, 'neu': 0.677, 'pos': 0.192, 'compound': 1.0}\n",
      "Michael Bennet: {'neg': 0.104, 'neu': 0.701, 'pos': 0.196, 'compound': 0.9993}\n",
      "Joe Biden: {'neg': 0.088, 'neu': 0.774, 'pos': 0.139, 'compound': 1.0}\n",
      "Kamala Harris: {'neg': 0.115, 'neu': 0.681, 'pos': 0.204, 'compound': 0.9999}\n",
      "John Hickenlooper: {'neg': 0.116, 'neu': 0.723, 'pos': 0.161, 'compound': 0.9931}\n",
      "Kirsten Gillibrand: {'neg': 0.133, 'neu': 0.692, 'pos': 0.175, 'compound': 0.995}\n",
      "Pete Buttigieg: {'neg': 0.122, 'neu': 0.697, 'pos': 0.181, 'compound': 1.0}\n",
      "Andrew Yang: {'neg': 0.102, 'neu': 0.692, 'pos': 0.206, 'compound': 1.0}\n",
      "Eric Swalwell: {'neg': 0.197, 'neu': 0.687, 'pos': 0.116, 'compound': -0.993}\n",
      "Marianne Williamson: {'neg': 0.143, 'neu': 0.672, 'pos': 0.185, 'compound': 0.9916}\n",
      "Steve Bullock: {'neg': 0.069, 'neu': 0.734, 'pos': 0.197, 'compound': 0.9992}\n",
      "Tom Steyer: {'neg': 0.126, 'neu': 0.718, 'pos': 0.156, 'compound': 0.999}\n",
      "Mike Bloomberg: {'neg': 0.137, 'neu': 0.713, 'pos': 0.15, 'compound': 0.9628}\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing()\n",
    "sentiment(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
